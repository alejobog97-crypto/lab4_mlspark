# %% [markdown]
# # Notebook 10: MLflow Tracking
#
# **Secci√≥n 16 - MLOps**: Registro de experimentos con MLflow
#
# **Objetivo**: Rastrear experimentos, m√©tricas y modelos con MLflow
#
# ## Conceptos clave:
# - **Experiment**: Agrupaci√≥n l√≥gica de runs (un proyecto)
# - **Run**: Una ejecuci√≥n individual (un modelo entrenado)
# - **Parameters**: Hiperpar√°metros registrados (regParam, maxIter, etc.)
# - **Metrics**: M√©tricas de rendimiento (RMSE, R¬≤, etc.)
# - **Artifacts**: Archivos guardados (modelos, gr√°ficos, etc.)
#
# ## Actividades:
# 1. Configurar MLflow tracking server
# 2. Registrar experimentos con hiperpar√°metros
# 3. Guardar m√©tricas y artefactos
# 4. Comparar runs en MLflow UI

# %%
from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import when, col
from pyspark.sql.functions import abs as spark_abs, col
from pyspark.ml.feature import StandardScaler, PCA, VectorAssembler
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.tuning import ParamGridBuilder
from delta import configure_spark_with_delta_pip
from pyspark.ml.feature import VectorAssembler
import numpy as np
import mlflow
import mlflow.spark

# %%
# %%
# Configurar SparkSession
builder = (
    SparkSession.builder
    .appName("SECOP_EDA")
    .master("spark://spark-master:7077")
    .config("spark.executor.memory", "2g")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

print(f"Spark Version: {spark.version}")


# %% [markdown]
# ## RETO 1: Configurar MLflow
#
# Objetivo: Conectarse al tracking server y crear un experimento
#
# Pregunta:
# ¬øPor qu√© es importante un tracking server centralizado
# en lugar de guardar m√©tricas en archivos locales?

# %%
import mlflow
from pyspark.sql.functions import col
from pyspark.ml.evaluation import RegressionEvaluator

# -----------------------------------------
# Configuraci√≥n de MLflow
# -----------------------------------------

# URI del tracking server (contenedor MLflow)
mlflow.set_tracking_uri("http://mlflow:5000")

# Nombre del experimento
experiment_name = "/SECOP_Contratos_Prediccion"
mlflow.set_experiment(experiment_name)

print(f"MLflow Tracking URI: {mlflow.get_tracking_uri()}")
print(f"Experimento activo: {experiment_name}")

# -----------------------------------------
# Respuesta conceptual:
#
# Un tracking server centralizado permite:
# - Centralizar m√©tricas, par√°metros y modelos de m√∫ltiples ejecuciones
# - Comparar experimentos entre diferentes usuarios o pipelines
# - Mantener trazabilidad y reproducibilidad de modelos
# - Evitar p√©rdida de informaci√≥n al trabajar en entornos distribuidos
# - Facilitar auditor√≠a, monitoreo y despliegue en producci√≥n
#
# Guardar m√©tricas en archivos locales no escala, no es colaborativo
# y dificulta la reproducibilidad en equipos de datos.
# -----------------------------------------


# %%
# Cargar datos listos para ML
df = spark.read.parquet("/opt/spark-data/processed/secop_ml_ready.parquet")

df = (
    df.withColumnRenamed("valor_del_contrato_num", "label")
      .withColumnRenamed("features_pca", "features")
      .filter(col("label").isNotNull())
)

# Split train / test
train, test = df.randomSplit([0.8, 0.2], seed=42)

print(f"Train: {train.count():,}")
print(f"Test:  {test.count():,}")


# %%
# Evaluador base
evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

# %% [markdown]
# ## RETO 2: Registrar un Experimento Baseline
#
# Objetivo: Entrenar un modelo SIN regularizaci√≥n y registrarlo en MLflow

# %%
import mlflow
from pyspark.ml.regression import LinearRegression

# -----------------------------------------
# Registrar experimento baseline en MLflow
# -----------------------------------------

with mlflow.start_run(run_name="baseline_no_regularization"):

    # -----------------------------
    # Hiperpar√°metros del modelo
    # -----------------------------
    reg_param = 0.0
    elastic_param = 0.0
    max_iter = 100

    # Log de hiperpar√°metros
    mlflow.log_param("model_type", "LinearRegression")
    mlflow.log_param("regParam", reg_param)
    mlflow.log_param("elasticNetParam", elastic_param)
    mlflow.log_param("maxIter", max_iter)

    # -----------------------------
    # Entrenamiento del modelo
    # -----------------------------
    lr = LinearRegression(
        featuresCol="features",
        labelCol="label",
        maxIter=max_iter,
        regParam=reg_param,
        elasticNetParam=elastic_param
    )

    model = lr.fit(train)

    # -----------------------------
    # Evaluaci√≥n
    # -----------------------------
    predictions = model.transform(test)
    rmse = evaluator.evaluate(predictions)

    # Log de m√©tricas
    mlflow.log_metric("rmse", rmse)

    # -----------------------------
    # Guardar modelo como artefacto
    # -----------------------------
    mlflow.spark.log_model(model, artifact_path="model")

    print(f"‚úì Experimento baseline registrado")
    print(f"  RMSE Test: ${rmse:,.2f}")

# %% [markdown]
# ## RETO 3: Registrar M√∫ltiples Experimentos
#
# Objetivo: Entrenar y registrar varios modelos con diferentes regularizaciones
# y compararlos en MLflow UI.

# %%
import mlflow
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# -----------------------------------------
# Evaluadores
# -----------------------------------------
evaluator_rmse = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse"
)

evaluator_mae = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="mae"
)

evaluator_r2 = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="r2"
)

# -----------------------------------------
# Configuraciones de experimentos
# -----------------------------------------
experiments = [
    {"name": "ridge_l2", "reg": 0.1, "elastic": 0.0, "type": "Ridge"},
    {"name": "lasso_l1", "reg": 0.1, "elastic": 1.0, "type": "Lasso"},
    {"name": "elasticnet", "reg": 0.1, "elastic": 0.5, "type": "ElasticNet"},
]

max_iter = 100

# -----------------------------------------
# Ejecutar experimentos
# -----------------------------------------
for exp in experiments:

    with mlflow.start_run(run_name=exp["name"]):

        print(f"\nEntrenando modelo: {exp['type']}")

        # -----------------------------
        # Log de par√°metros
        # -----------------------------
        mlflow.log_param("model_type", "LinearRegression")
        mlflow.log_param("regularization_type", exp["type"])
        mlflow.log_param("regParam", exp["reg"])
        mlflow.log_param("elasticNetParam", exp["elastic"])
        mlflow.log_param("maxIter", max_iter)

        # -----------------------------
        # Entrenamiento
        # -----------------------------
        lr = LinearRegression(
            featuresCol="features",
            labelCol="label",
            maxIter=max_iter,
            regParam=exp["reg"],
            elasticNetParam=exp["elastic"]
        )

        model = lr.fit(train)

        # -----------------------------
        # Evaluaci√≥n
        # -----------------------------
        predictions = model.transform(test)

        rmse = evaluator_rmse.evaluate(predictions)
        mae = evaluator_mae.evaluate(predictions)
        r2 = evaluator_r2.evaluate(predictions)

        # -----------------------------
        # Log de m√©tricas
        # -----------------------------
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("r2", r2)

        # -----------------------------
        # Guardar modelo
        # -----------------------------
        mlflow.spark.log_model(model, artifact_path="model")

        # -----------------------------
        # Output informativo
        # -----------------------------
        print(f"‚úì {exp['type']} registrado en MLflow")
        print(f"  RMSE: ${rmse:,.2f}")
        print(f"  MAE : ${mae:,.2f}")
        print(f"  R¬≤  : {r2:.4f}")


# %% [markdown]
# ## RETO 4: Explorar MLflow UI
#
# **URL de MLflow UI**:
# üëâ http://localhost:5000
#
# **Pasos realizados**:
# 1. Se accedi√≥ a la interfaz web de MLflow
# 2. Se seleccion√≥ el experimento: `/SECOP_Contratos_Prediccion`
# 3. Se compararon los runs registrados (Baseline, Ridge, Lasso, ElasticNet)
# 4. Se ordenaron los resultados por la m√©trica **RMSE**
# 5. Se revisaron par√°metros, m√©tricas y artefactos de cada run
#
# ---
#
# ### Resultados Observados
#
# **Mejor modelo en MLflow UI**:
# - Tipo de modelo: ___________________________
# - Regularizaci√≥n: ___________________________
#
# **RMSE del mejor modelo**:
# - RMSE Test: $____________________
#
# ---
#
# ### An√°lisis
#
# - ¬øExiste correlaci√≥n entre regularizaci√≥n y rendimiento?
#   - ‚òê S√≠
#   - ‚òê No
#
# **Observaciones**:
# - La regularizaci√≥n ayud√≥ a:
#   - ‚òê Reducir overfitting
#   - ‚òê Mejorar generalizaci√≥n
#   - ‚òê No tuvo impacto significativo
#
# - Comparando modelos:
#   - Ridge (L2): ______________________________
#   - Lasso (L1): ______________________________
#   - ElasticNet: ______________________________
#
# ---
#
# ### Comunicaci√≥n con el equipo
#
# **¬øC√≥mo compartir estos resultados?**
# - ‚òê Enlace directo al experimento en MLflow UI
# - ‚òê Screenshot comparativo de m√©tricas
# - ‚òê Exportar m√©tricas a reporte (PDF / PPT)
# - ‚òê Registrar conclusiones en documentaci√≥n t√©cnica
#
# **Recomendaci√≥n final**:
# _______________________________________________________
#
# ---
#
# ‚úîÔ∏è Conclusi√≥n:
# MLflow permite comparar modelos de forma objetiva,
# reproducible y auditable, facilitando la toma de decisiones
# y el trabajo colaborativo en equipos de datos.

# %%
# RETO 5: Agregar Artefactos Personalizados

import mlflow
import mlflow.spark
import matplotlib.pyplot as plt
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

with mlflow.start_run(run_name="model_with_artifacts"):

    # -----------------------------
    # 1. Entrenar modelo (ejemplo)
    # -----------------------------
    lr = LinearRegression(
        featuresCol="features",
        labelCol="label",
        maxIter=100,
        regParam=0.1,
        elasticNetParam=0.0
    )

    model = lr.fit(train)

    # -----------------------------
    # 2. Evaluaci√≥n
    # -----------------------------
    predictions = model.transform(test)

    evaluator_rmse = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="rmse"
    )
    evaluator_mae = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="mae"
    )
    evaluator_r2 = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="r2"
    )

    rmse = evaluator_rmse.evaluate(predictions)
    mae = evaluator_mae.evaluate(predictions)
    r2 = evaluator_r2.evaluate(predictions)

    # -----------------------------
    # 3. Log de m√©tricas
    # -----------------------------
    mlflow.log_param("regParam", 0.1)
    mlflow.log_param("elasticNetParam", 0.0)
    mlflow.log_param("maxIter", 100)
    mlflow.log_param("model_type", "LinearRegression")

    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("mae", mae)
    mlflow.log_metric("r2", r2)

    # -----------------------------
    # 4. Artefacto: Reporte de texto
    # -----------------------------
    report = f"""
    REPORTE DEL MODELO
    ==================
    Modelo: Regresi√≥n Lineal (Ridge)

    M√©tricas:
    - RMSE: ${rmse:,.2f}
    - MAE:  ${mae:,.2f}
    - R¬≤:   {r2:.4f}

    Observaci√≥n:
    Este modelo incluye regularizaci√≥n L2 para reducir overfitting
    y mejorar la generalizaci√≥n en datos no vistos.
    """

    mlflow.log_text(report, "model_report.txt")

    # -----------------------------
    # 5. (Bonus) Artefacto gr√°fico
    # -----------------------------
    # Convertir muestra a pandas para graficar
    pdf = predictions.select("label", "prediction").sample(0.1, seed=42).toPandas()

    plt.figure(figsize=(6, 6))
    plt.scatter(pdf["label"], pdf["prediction"], alpha=0.5)
    plt.plot(
        [pdf["label"].min(), pdf["label"].max()],
        [pdf["label"].min(), pdf["label"].max()],
        "r--"
    )
    plt.xlabel("Valor Real")
    plt.ylabel("Valor Predicho")
    plt.title("Predicciones vs Valores Reales")
    plt.grid(True)

    plot_path = "/tmp/predicciones_vs_reales.png"
    plt.savefig(plot_path)
    plt.close()

    mlflow.log_artifact(plot_path)

    # -----------------------------
    # 6. Guardar modelo
    # -----------------------------
    mlflow.spark.log_model(model, "model")

    print(f"Run registrado con RMSE = ${rmse:,.2f}")


# %% [markdown]
# ## Preguntas de Reflexi√≥n
#
# 1. **¬øQu√© ventajas tiene MLflow sobre guardar m√©tricas en archivos CSV?**
#
# MLflow ofrece trazabilidad completa de los experimentos, permitiendo
# comparar modelos, par√°metros, m√©tricas y artefactos en un solo lugar.
# A diferencia de archivos CSV, MLflow:
# - Centraliza los experimentos (multiusuario y multi-entorno)
# - Mantiene el historial completo de ejecuciones
# - Permite reproducibilidad exacta de modelos
# - Facilita la comparaci√≥n visual en la UI
# - Gestiona modelos, m√©tricas y artefactos de forma estructurada
#
#
# 2. **¬øC√≥mo implementar√≠as MLflow en un proyecto de equipo?**
#
# Implementar√≠a MLflow como un servicio centralizado accesible para todo el equipo:
# - Un tracking server compartido (Docker / Kubernetes)
# - Convenciones de nombres para experimentos y runs
# - Registro obligatorio de par√°metros, m√©tricas y modelos
# - Integraci√≥n con pipelines de CI/CD
# - Uso del Model Registry para controlar versiones
# - Roles claros (Data Scientist, Reviewer, Product Owner)
#
#
# 3. **¬øQu√© artefactos adicionales guardar√≠as adem√°s del modelo?**
#
# Adem√°s del modelo entrenado, guardar√≠a:
# - Reportes de m√©tricas (TXT / JSON)
# - Gr√°ficos (residuos, ROC, predicci√≥n vs real)
# - Esquema de features
# - Versiones de datasets o hashes
# - C√≥digo del entrenamiento
# - Configuraci√≥n del entorno (requirements.txt / conda.yaml)
#
#
# 4. **¬øC√≥mo automatizar√≠as el registro de experimentos?**
#
# Automatizar√≠a el registro integrando MLflow en:
# - Pipelines de entrenamiento (scripts o notebooks)
# - Jobs programados (Airflow / Prefect / cron)
# - CI/CD (GitHub Actions, GitLab CI)
# - Uso de templates de entrenamiento con MLflow incluido por defecto
#
# De esta forma, cada entrenamiento queda registrado autom√°ticamente
# sin depender de acciones manuales.


# %%
print("\n" + "="*60)
print("RESUMEN MLFLOW TRACKING")
print("="*60)
print("Verifica que hayas completado:")
print("  [ ] Configurado MLflow tracking server")
print("  [ ] Registrado experimento baseline")
print("  [ ] Registrado al menos 3 experimentos adicionales")
print("  [ ] Explorado MLflow UI")
print("  [ ] Comparado m√©tricas entre runs")
print(f"  [ ] Accede a MLflow UI: http://localhost:5000")
print("="*60)

# %%
spark.stop()
