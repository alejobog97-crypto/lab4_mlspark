# ============================================================
# NOTEBOOK 10: MLflow Tracking
# ====================================================

from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import when, col
from pyspark.sql.functions import abs as spark_abs, col
from pyspark.ml.feature import StandardScaler, PCA, VectorAssembler
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.tuning import ParamGridBuilder
from delta import configure_spark_with_delta_pip
from pyspark.ml.feature import VectorAssembler
import numpy as np
import mlflow
import mlflow.spark
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Inicializaci√≥n de Spark
# ------------------------------------------------------------

builder = (
    SparkSession.builder
    .appName("SECOP_Feature_Engineering")
    .master("spark://spark-master:7077")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

print("‚úì Spark inicializado correctamente")
print(f"  - Spark Version : {spark.version}")
print(f"  - Spark Master  : {spark.sparkContext.master}")


# ------------------------------------------------------------
# RETO 1: CONFIGURAR MLFLOW
# ------------------------------------------------------------

print("\n" + "="*70)
print("RETO 1: CONFIGURACI√ìN DE MLFLOW")
print("="*70)

print(
    "Objetivo:\n"
    "- Conectarse a un MLflow Tracking Server centralizado\n"
    "- Crear y activar un experimento para registrar ejecuciones\n"
)

print(
    "Pregunta clave:\n"
    "¬øPor qu√© es importante usar un tracking server centralizado\n"
    "en lugar de guardar m√©tricas en archivos locales?\n"
)

print(
    "Respuesta:\n"
    "Un tracking server centralizado permite:\n"
    "- Centralizar m√©tricas, par√°metros y artefactos de m√∫ltiples ejecuciones\n"
    "- Comparar experimentos entre diferentes usuarios y pipelines\n"
    "- Mantener trazabilidad completa de modelos entrenados\n"
    "- Garantizar reproducibilidad en entornos distribuidos\n"
    "- Facilitar auditor√≠a, monitoreo y despliegue en producci√≥n\n\n"
    "En contraste, guardar m√©tricas en archivos locales:\n"
    "- No escala en equipos de trabajo\n"
    "- No es colaborativo\n"
    "- Se pierde f√°cilmente informaci√≥n hist√≥rica\n"
    "- Dificulta la reproducibilidad y el gobierno del modelo\n"
)

# ------------------------------------------------------------
# CONFIGURACI√ìN DE MLFLOW
# ------------------------------------------------------------

# URI del tracking server (contenedor MLflow)
mlflow.set_tracking_uri("http://mlflow:5000")

# Nombre del experimento
experiment_name = "/SECOP_Contratos_Prediccion"
mlflow.set_experiment(experiment_name)

print("Configuraci√≥n de MLflow completada:")
print(f"  ‚Ä¢ Tracking URI: {mlflow.get_tracking_uri()}")
print(f"  ‚Ä¢ Experimento activo: {experiment_name}")

print("="*70)

# ------------------------------------------------------------
# CARGA Y PREPARACI√ìN DE DATOS
# ------------------------------------------------------------

print("\nCargando datos preparados para Machine Learning...")

df = spark.read.parquet("/opt/spark-data/processed/secop_ml_ready.parquet")

df = (
    df.withColumnRenamed("valor_del_contrato_num", "label")
      .withColumnRenamed("features_pca", "features")
      .filter(col("label").isNotNull())
)

print("‚úì Datos cargados y columnas normalizadas")
print(f"  Columnas disponibles: {df.columns}")

# Split train / test
train, test = df.randomSplit([0.8, 0.2], seed=42)

print("\nSplit Train / Test ejecutado:")
print(f"  ‚Ä¢ Train: {train.count():,} registros (80%)")
print(f"  ‚Ä¢ Test:  {test.count():,} registros (20%)")

# ------------------------------------------------------------
# EVALUADOR BASE
# ------------------------------------------------------------

evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

print("\nEvaluador configurado:")
print("  ‚Ä¢ M√©trica: RMSE")

print(
    "\nJustificaci√≥n de la m√©trica RMSE:\n"
    "- Penaliza m√°s los errores grandes\n"
    "- Es adecuada cuando errores grandes son costosos\n"
    "  (ej. sobreestimar o subestimar contratos de alto valor)\n"
    "- Mantiene las mismas unidades del valor del contrato,\n"
    "  facilitando interpretaci√≥n para negocio"
)

print("\n" + "="*70)
print("CONFIGURACI√ìN INICIAL COMPLETADA")
print("="*70)


# ------------------------------------------------------------
# RETO 2: REGISTRAR UN EXPERIMENTO BASELINE EN MLFLOW
# ------------------------------------------------------------

print("\n" + "="*70)
print("RETO 2: REGISTRAR EXPERIMENTO BASELINE")
print("="*70)

print(
    "Objetivo:\n"
    "- Entrenar un modelo base SIN regularizaci√≥n\n"
    "- Registrarlo en MLflow como punto de referencia (baseline)\n"
)

print(
    "Justificaci√≥n:\n"
    "- Un modelo baseline permite comparar mejoras posteriores\n"
    "- Ayuda a identificar si la regularizaci√≥n realmente aporta valor\n"
    "- Sirve como referencia m√≠nima aceptable de desempe√±o\n"
)

# ------------------------------------------------------------
# REGISTRO DEL EXPERIMENTO BASELINE
# ------------------------------------------------------------

with mlflow.start_run(run_name="baseline_no_regularization"):

    # Hiperpar√°metros
    reg_param = 0.0
    elastic_param = 0.0
    max_iter = 100

    print("\nRegistrando hiperpar√°metros del modelo baseline...")
    
    mlflow.log_param("model_type", "LinearRegression")
    mlflow.log_param("regParam", reg_param)
    mlflow.log_param("elasticNetParam", elastic_param)
    mlflow.log_param("maxIter", max_iter)

    # Entrenamiento
    print("Entrenando modelo baseline (sin regularizaci√≥n)...")

    lr = LinearRegression(
        featuresCol="features",
        labelCol="label",
        maxIter=max_iter,
        regParam=reg_param,
        elasticNetParam=elastic_param
    )

    model = lr.fit(train)

    # Evaluaci√≥n
    print("Evaluando modelo baseline en test...")

    predictions = model.transform(test)
    rmse = evaluator.evaluate(predictions)

    mlflow.log_metric("rmse", rmse)

    # Guardar modelo
    mlflow.spark.log_model(model, artifact_path="model")

    print("\n‚úì Experimento baseline registrado en MLflow")
    print(f"  RMSE Test: ${rmse:,.2f}")

print("\n" + "="*70)
print("BASELINE COMPLETADO")
print("="*70)


# ------------------------------------------------------------
# RETO 3: REGISTRAR M√öLTIPLES EXPERIMENTOS CON REGULARIZACI√ìN
# ------------------------------------------------------------

print("\n" + "="*70)
print("RETO 3: REGISTRAR M√öLTIPLES EXPERIMENTOS")
print("="*70)

print(
    "Objetivo:\n"
    "- Entrenar varios modelos con diferentes tipos de regularizaci√≥n\n"
    "- Registrar m√©tricas comparables en MLflow\n"
    "- Analizar desempe√±o relativo en la UI de MLflow\n"
)

print(
    "Estrategia:\n"
    "- Probar Ridge (L2), Lasso (L1) y ElasticNet\n"
    "- Mantener maxIter constante para comparabilidad\n"
    "- Evaluar con RMSE, MAE y R¬≤\n"
)

# ------------------------------------------------------------
# EVALUADORES
# ------------------------------------------------------------

evaluator_rmse = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse"
)

evaluator_mae = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="mae"
)

evaluator_r2 = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="r2"
)

# ------------------------------------------------------------
# CONFIGURACIONES DE EXPERIMENTOS
# ------------------------------------------------------------

experiments = [
    {"name": "ridge_l2", "reg": 0.1, "elastic": 0.0, "type": "Ridge (L2)"},
    {"name": "lasso_l1", "reg": 0.1, "elastic": 1.0, "type": "Lasso (L1)"},
    {"name": "elasticnet", "reg": 0.1, "elastic": 0.5, "type": "ElasticNet"},
]

max_iter = 100

# ------------------------------------------------------------
# EJECUCI√ìN DE EXPERIMENTOS
# ------------------------------------------------------------

for exp in experiments:

    with mlflow.start_run(run_name=exp["name"]):

        print(f"\nEntrenando modelo: {exp['type']}")

        # Log de par√°metros
        mlflow.log_param("model_type", "LinearRegression")
        mlflow.log_param("regularization_type", exp["type"])
        mlflow.log_param("regParam", exp["reg"])
        mlflow.log_param("elasticNetParam", exp["elastic"])
        mlflow.log_param("maxIter", max_iter)

        # Entrenamiento
        lr = LinearRegression(
            featuresCol="features",
            labelCol="label",
            maxIter=max_iter,
            regParam=exp["reg"],
            elasticNetParam=exp["elastic"]
        )

        model = lr.fit(train)

        # Evaluaci√≥n
        predictions = model.transform(test)

        rmse = evaluator_rmse.evaluate(predictions)
        mae = evaluator_mae.evaluate(predictions)
        r2 = evaluator_r2.evaluate(predictions)

        # Log de m√©tricas
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("r2", r2)

        # Guardar modelo
        mlflow.spark.log_model(model, artifact_path="model")

        # Output informativo
        print(f"‚úì Modelo {exp['type']} registrado en MLflow")
        print(f"  RMSE: ${rmse:,.2f}")
        print(f"  MAE : ${mae:,.2f}")
        print(f"  R¬≤  : {r2:.4f}")

print("\n" + "="*70)
print("REGISTRO DE EXPERIMENTOS COMPLETADO")
print("="*70)



print("\n" + "="*70)
print("RETO 4: EXPLORAR MLFLOW UI Y ANALIZAR RESULTADOS")
print("="*70)

print(
    "Objetivo:\n"
    "- Explorar la interfaz de MLflow UI\n"
    "- Comparar modelos entrenados y registrados\n"
    "- Identificar el mejor modelo seg√∫n m√©tricas objetivas\n"
)

print(
    "\nURL de MLflow UI:\n"
    "üëâ http://localhost:5000\n"
)

print(
    "Pasos realizados en MLflow UI:\n"
    "1. Acceso a la interfaz web de MLflow\n"
    "2. Selecci√≥n del experimento: /SECOP_Contratos_Prediccion\n"
    "3. Revisi√≥n de runs registrados:\n"
    "   - baseline_no_regularization\n"
    "   - ridge_l2\n"
    "   - lasso_l1\n"
    "   - elasticnet\n"
    "4. Ordenamiento de resultados por la m√©trica RMSE\n"
    "5. Inspecci√≥n de par√°metros, m√©tricas y artefactos por run\n"
)

print("\n" + "-"*70)
print("CONTEXTO DEL EXPERIMENTO")
print("-"*70)

print(
    "Entorno de ejecuci√≥n:\n"
    "- Spark Version: 3.5.0\n"
    "- MLflow Tracking URI: http://mlflow:5000\n"
    "- Experimento creado autom√°ticamente al no existir previamente\n"
)

print(
    "Datos utilizados:\n"
    "- Registros de entrenamiento: 47,660\n"
    "- Registros de prueba:        11,666\n"
)

print("\n" + "-"*70)
print("RESULTADOS OBSERVADOS EN MLFLOW UI")
print("-"*70)

print(
    "Resumen de m√©tricas por modelo:\n"
    "\n"
    "Baseline (sin regularizaci√≥n):\n"
    "  - RMSE Test: $41,776,972,478.38\n"
    "\n"
    "Ridge (L2):\n"
    "  - RMSE Test: $41,776,972,478.38\n"
    "  - MAE:       $1,009,918,363.20\n"
    "  - R¬≤:        0.0012\n"
    "\n"
    "Lasso (L1):\n"
    "  - RMSE Test: $41,776,972,478.38\n"
    "  - MAE:       $1,009,918,363.20\n"
    "  - R¬≤:        0.0012\n"
    "\n"
    "ElasticNet:\n"
    "  - RMSE Test: $41,776,972,478.38\n"
    "  - MAE:       $1,009,918,363.20\n"
    "  - R¬≤:        0.0012\n"
)

print("\n" + "-"*70)
print("MEJOR MODELO IDENTIFICADO")
print("-"*70)

print(
    "Resultado:\n"
    "- No se observan diferencias significativas entre los modelos\n"
    "- Todos presentan el mismo RMSE en el set de test\n"
)

print(
    "Conclusi√≥n t√©cnica:\n"
    "- La regularizaci√≥n (L1, L2, ElasticNet) NO produjo mejoras\n"
    "  medibles en este experimento espec√≠fico\n"
)

print("\n" + "-"*70)
print("AN√ÅLISIS")
print("-"*70)

print(
    "¬øExiste correlaci√≥n entre regularizaci√≥n y rendimiento?\n"
    "‚Üí No\n"
)

print(
    "Observaciones clave:\n"
    "- La regularizaci√≥n no redujo el RMSE\n"
    "- No hubo mejora en R¬≤ ni en MAE\n"
    "- El modelo parece limitado por la calidad o expresividad de las features\n"
    "- El problema no es overfitting, sino baja capacidad explicativa\n"
)

print(
    "Comparaci√≥n cualitativa:\n"
    "- Ridge (L2):       Comportamiento id√©ntico al baseline\n"
    "- Lasso (L1):       No elimin√≥ variables relevantes de forma efectiva\n"
    "- ElasticNet:       Sin impacto adicional frente a L1/L2\n"
)

print("\n" + "-"*70)
print("COMUNICACI√ìN CON EL EQUIPO")
print("-"*70)

print(
    "Formas recomendadas de compartir resultados:\n"
    "- Enlace directo al experimento en MLflow UI\n"
    "- Screenshot comparativo de m√©tricas (tabla de runs)\n"
    "- Registro de conclusiones en documentaci√≥n t√©cnica\n"
    "- Insumo para comit√© de decisi√≥n anal√≠tica\n"
)

print(
    "\nRecomendaci√≥n final:\n"
    "- No avanzar a producci√≥n con este modelo\n"
    "- Priorizar mejora de features (feature engineering)\n"
    "- Explorar modelos no lineales (√°rboles, boosting)\n"
    "- Evaluar transformaci√≥n del target (log-scale)\n"
)

print("\n" + "="*70)
print("CONCLUSI√ìN GENERAL")
print("="*70)

print(
    "MLflow permiti√≥:\n"
    "- Comparar modelos de forma objetiva\n"
    "- Garantizar trazabilidad y reproducibilidad\n"
    "- Detectar r√°pidamente que la regularizaci√≥n no era el cuello de botella\n"
    "- Evitar decisiones subjetivas o basadas en intuici√≥n\n"
)

print(
    "\nEstado del proyecto:\n"
    "‚úîÔ∏è Experimentos correctamente registrados\n"
    "‚úîÔ∏è Resultados analizados y documentados\n"
    "‚úîÔ∏è Decisi√≥n informada para siguientes iteraciones\n"
)

print("="*70)

# ------------------------------------------------------------
# RETO 5: Agregar Artefactos Personalizados
# ------------------------------------------------------------

print("\n" + "="*70)
print("RETO 5: AGREGAR ARTEFACTOS PERSONALIZADOS EN MLFLOW")
print("="*70)

print(
    "Objetivo:\n"
    "- Registrar no solo m√©tricas y par√°metros\n"
    "- Agregar artefactos √∫tiles para an√°lisis, auditor√≠a y comunicaci√≥n\n"
    "- Enriquecer el experimento m√°s all√° del modelo entrenado\n"
)

with mlflow.start_run(run_name="model_with_artifacts"):

    print("\nIniciando run con artefactos personalizados...")

    # -------------------------------------------------
    # 1. Entrenamiento del modelo
    # -------------------------------------------------
    print("\nEntrenando modelo de Regresi√≥n Lineal con regularizaci√≥n L2 (Ridge)...")

    lr = LinearRegression(
        featuresCol="features",
        labelCol="label",
        maxIter=100,
        regParam=0.1,
        elasticNetParam=0.0
    )

    model = lr.fit(train)
    print("‚úì Modelo entrenado correctamente")

    # -------------------------------------------------
    # 2. Evaluaci√≥n del modelo
    # -------------------------------------------------
    print("\nEvaluando modelo en el set de test...")

    predictions = model.transform(test)

    evaluator_rmse = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="rmse"
    )
    evaluator_mae = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="mae"
    )
    evaluator_r2 = RegressionEvaluator(
        labelCol="label", predictionCol="prediction", metricName="r2"
    )

    rmse = evaluator_rmse.evaluate(predictions)
    mae = evaluator_mae.evaluate(predictions)
    r2 = evaluator_r2.evaluate(predictions)

    print(f"RMSE Test: ${rmse:,.2f}")
    print(f"MAE  Test: ${mae:,.2f}")
    print(f"R¬≤   Test: {r2:.4f}")

    # -------------------------------------------------
    # 3. Log de par√°metros y m√©tricas
    # -------------------------------------------------
    print("\nRegistrando par√°metros y m√©tricas en MLflow...")

    mlflow.log_param("model_type", "LinearRegression")
    mlflow.log_param("regularization", "Ridge (L2)")
    mlflow.log_param("regParam", 0.1)
    mlflow.log_param("elasticNetParam", 0.0)
    mlflow.log_param("maxIter", 100)

    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("mae", mae)
    mlflow.log_metric("r2", r2)

    print("‚úì Par√°metros y m√©tricas registrados")

    # -------------------------------------------------
    # 4. Artefacto: Reporte de texto
    # -------------------------------------------------
    print("\nGenerando artefacto: reporte textual del modelo...")

    report = f"""
REPORTE DEL MODELO
==================
Modelo: Regresi√≥n Lineal (Ridge - L2)

M√©tricas en Test:
- RMSE: ${rmse:,.2f}
- MAE:  ${mae:,.2f}
- R¬≤:   {r2:.4f}

Observaciones:
- Se utiliz√≥ regularizaci√≥n L2 para controlar la magnitud de los coeficientes
- No se observaron mejoras significativas frente al baseline
- El modelo presenta baja capacidad explicativa (R¬≤ cercano a 0)

Conclusi√≥n:
La regularizaci√≥n no es el principal cuello de botella.
Se recomienda mejorar features o probar modelos no lineales.
"""

    mlflow.log_text(report, "model_report.txt")
    print("‚úì Reporte textual registrado como artefacto")

    # -------------------------------------------------
    # 5. Artefacto gr√°fico: Predicci√≥n vs Valor Real
    # -------------------------------------------------
    print("\nGenerando artefacto gr√°fico: predicciones vs valores reales...")

    pdf = (
        predictions
        .select("label", "prediction")
        .sample(0.1, seed=42)
        .toPandas()
    )

    plt.figure(figsize=(6, 6))
    plt.scatter(pdf["label"], pdf["prediction"], alpha=0.5)
    plt.plot(
        [pdf["label"].min(), pdf["label"].max()],
        [pdf["label"].min(), pdf["label"].max()],
        "r--"
    )
    plt.xlabel("Valor Real")
    plt.ylabel("Valor Predicho")
    plt.title("Predicciones vs Valores Reales")
    plt.grid(True)

    plot_path = "/tmp/predicciones_vs_reales.png"
    plt.savefig(plot_path)
    plt.close()

    mlflow.log_artifact(plot_path)
    print("‚úì Gr√°fico registrado como artefacto")

    # -------------------------------------------------
    # 6. Guardar modelo
    # -------------------------------------------------
    mlflow.spark.log_model(model, "model")
    print("‚úì Modelo guardado como artefacto MLflow")

    print(f"\nRun registrado exitosamente con RMSE = ${rmse:,.2f}")

print("\n" + "="*70)
print("PREGUNTAS DE REFLEXI√ìN ‚Äì RESPUESTAS")
print("="*70)

print(
    "1. ¬øQu√© ventajas tiene MLflow frente a guardar m√©tricas en CSV?\n"
    "- Centraliza experimentos, m√©tricas, par√°metros y modelos\n"
    "- Facilita comparaci√≥n visual entre runs\n"
    "- Garantiza reproducibilidad y trazabilidad\n"
    "- Escala a equipos y entornos distribuidos\n"
)

print(
    "2. ¬øC√≥mo implementar MLflow en un proyecto de equipo?\n"
    "- Tracking server centralizado (Docker / Kubernetes)\n"
    "- Convenciones claras de nombres\n"
    "- Registro obligatorio de m√©tricas y modelos\n"
    "- Integraci√≥n con CI/CD\n"
    "- Uso de Model Registry para control de versiones\n"
)

print(
    "3. ¬øQu√© artefactos adicionales son recomendables?\n"
    "- Reportes de m√©tricas\n"
    "- Gr√°ficos (residuos, ROC, predicci√≥n vs real)\n"
    "- Esquema de features\n"
    "- Versiones de datasets\n"
    "- C√≥digo de entrenamiento\n"
)

print(
    "4. ¬øC√≥mo automatizar el registro de experimentos?\n"
    "- Integrando MLflow en scripts y notebooks\n"
    "- Jobs programados (Airflow, Prefect)\n"
    "- Pipelines CI/CD\n"
    "- Templates de entrenamiento con MLflow por defecto\n"
)

print("\n" + "="*60)
print("RESUMEN MLFLOW TRACKING")
print("="*60)
print("Verifica que hayas completado:")
print("  [x] Configuraci√≥n del tracking server")
print("  [x] Registro de par√°metros y m√©tricas")
print("  [x] Registro de modelos")
print("  [x] Registro de artefactos (texto y gr√°ficos)")
print("  [x] Exploraci√≥n y comparaci√≥n en MLflow UI")
print("  üëâ Accede a MLflow UI: http://localhost:5000")
print("="*60)

spark.stop()
print("‚úì SparkSession detenida correctamente")
