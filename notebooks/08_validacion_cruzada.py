# %% [markdown]
# # Notebook 08: Validaci√≥n Cruzada (K-Fold)
#
# **Secci√≥n 15 - Tuning**: Cross-validation para evitar overfitting
#
# **Objetivo**: Implementar K-Fold Cross-Validation
#
# ## Conceptos clave:
# - Divide datos en K folds (subconjuntos)
# - Entrena K veces, usando diferente fold como validaci√≥n
# - Promedia m√©tricas para obtener estimaci√≥n robusta
#
# ## Actividades:
# 1. Entender el concepto de K-Fold
# 2. Configurar CrossValidator en Spark ML
# 3. Combinar con ParamGrid para b√∫squeda de hiperpar√°metros
# 4. Analizar resultados

# %%
from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import when, col
from pyspark.sql.functions import abs as spark_abs, col
from pyspark.ml.feature import StandardScaler, PCA, VectorAssembler
from pyspark.ml import Pipeline, PipelineModel
from delta import configure_spark_with_delta_pip
from pyspark.ml.feature import VectorAssembler
import numpy as np

# %%
# Configurar SparkSession
builder = (
    SparkSession.builder
    .appName("SECOP_EDA")
    .master("spark://spark-master:7077")
    .config("spark.executor.memory", "2g")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

print(f"Spark Version: {spark.version}")

# %%
# Cargar datos
df = spark.read.parquet("/opt/spark-data/processed/secop_ml_ready.parquet")
df = df.withColumnRenamed("valor_del_contrato_num", "label") \
       .withColumnRenamed("features_pca", "features") \
       .filter(col("label").isNotNull())

train, test = df.randomSplit([0.8, 0.2], seed=42)

print(f"Train: {train.count():,}")
print(f"Test: {test.count():,}")

# %%
# RETO 1: Entender K-Fold Cross-Validation
#
# Supongamos K = 5

# 1. ¬øEn cu√°ntos subconjuntos se dividen los datos de train?
# ‚Üí En 5 subconjuntos (folds) del mismo tama√±o (aprox.).

# 2. ¬øCu√°ntos modelos se entrenan en total?
# ‚Üí Se entrenan 5 modelos.
#   En cada iteraci√≥n, uno de los folds act√∫a como validaci√≥n
#   y los otros 4 como entrenamiento.

# 3. ¬øQu√© porcentaje de datos se usa para validaci√≥n en cada iteraci√≥n?
# ‚Üí 1/K del total de los datos de entrenamiento.
# ‚Üí Para K=5: 20% validaci√≥n y 80% entrenamiento en cada iteraci√≥n.

# 4. ¬øQu√© m√©trica se reporta al final?
# ‚Üí El promedio (y a veces la desviaci√≥n est√°ndar) de la m√©trica
#   evaluada en cada fold (por ejemplo: RMSE promedio).

# ¬øPor qu√© K-Fold es mejor que un simple train/test split?
#
# - Reduce la dependencia de una sola partici√≥n aleatoria
# - Usa todos los datos tanto para entrenamiento como para validaci√≥n
# - Produce m√©tricas m√°s estables y confiables
# - Detecta mejor overfitting y underfitting
# - Es especialmente √∫til cuando el dataset no es muy grande


# %%
# RETO 2: Crear el Modelo Base y Evaluador
#
# Objetivo:
# - Definir un modelo base de regresi√≥n lineal
# - Definir un evaluador para comparar modelos

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Modelo base de Regresi√≥n Lineal (sin regularizaci√≥n expl√≠cita)
lr = LinearRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=100
)

print("‚úì Modelo base LinearRegression creado")
print(f"  featuresCol: {lr.getFeaturesCol()}")
print(f"  labelCol: {lr.getLabelCol()}")
print(f"  maxIter: {lr.getMaxIter()}")

# Evaluador del modelo
# Usamos RMSE porque penaliza m√°s los errores grandes
evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

print("‚úì Evaluador configurado")
print("  M√©trica: RMSE")

# Reflexi√≥n (comentario para el notebook):
#
# - RMSE es √∫til cuando los errores grandes son costosos (ej. contratos de alto valor)
# - MAE podr√≠a usarse si se quiere tratar todos los errores por igual
# - R¬≤ es complementario, pero no siempre suficiente para comparar modelos

# %%
# RETO 3: Construir el ParamGrid
#
# Objetivo:
# - Definir combinaciones de hiperpar√°metros para Cross-Validation
# - Explorar distintos niveles y tipos de regularizaci√≥n

from pyspark.ml.tuning import ParamGridBuilder

# Definici√≥n del grid de hiperpar√°metros
param_grid = (
    ParamGridBuilder()
    # Œª (lambda): fuerza de regularizaci√≥n
    .addGrid(lr.regParam, [0.01, 0.1, 1.0])
    
    # Œ± (alpha): tipo de regularizaci√≥n
    # 0.0 = Ridge (L2)
    # 0.5 = ElasticNet
    # 1.0 = Lasso (L1)
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
    
    .build()
)

# N√∫mero de combinaciones
num_combinations = len(param_grid)
print(f"Combinaciones en el grid: {num_combinations}")

# Si usamos K-Fold Cross-Validation
K = 5
total_models = num_combinations * K
print(f"Total de modelos a entrenar: {total_models}")

# Explicaci√≥n (comentario para el notebook):
#
# - 3 valores de regParam √ó 3 valores de elasticNetParam = 9 combinaciones
# - Con K = 5 folds:
#   üëâ 9 √ó 5 = 45 modelos entrenados en total
#
# Esto explica por qu√© Cross-Validation puede ser computacionalmente costoso


# %%
# RETO 4: Configurar CrossValidator
#
# Objetivo:
# - Ensamblar el proceso de Cross-Validation
# - Entrenar m√∫ltiples modelos autom√°ticamente
# - Seleccionar el mejor seg√∫n la m√©trica (RMSE)

from pyspark.ml.tuning import CrossValidator

# Elecci√≥n de K
# K = 5 ‚Üí balance cl√°sico entre robustez y costo computacional
K = 5

# Configuraci√≥n del CrossValidator
crossval = CrossValidator(
    estimator=lr,                     # Modelo base
    estimatorParamMaps=param_grid,    # Grid de hiperpar√°metros
    evaluator=evaluator,              # M√©trica (RMSE)
    numFolds=K,                       # K-Fold Cross-Validation
    seed=42                           # Reproducibilidad
)

print(f"‚úì Cross-Validation configurado con K={K} folds")
print(f"‚úì Combinaciones de hiperpar√°metros: {len(param_grid)}")
print(f"‚úì Total de modelos a entrenar: {len(param_grid) * K}")

# Explicaci√≥n (comentario para el notebook):
#
# - Usamos K=5 porque:
#   ‚úîÔ∏è Reduce la varianza del error
#   ‚úîÔ∏è No es tan costoso como K=10
#   ‚úîÔ∏è Es est√°ndar en problemas reales
#
# - Total de modelos entrenados:
#   combinaciones √ó K = {len(param_grid)} √ó {K}
#
# ‚ö†Ô∏è En datasets muy grandes, este n√∫mero puede crecer r√°pidamente


# %%
# RETO 5: Ejecutar Cross-Validation y Analizar Resultados
#
# Objetivo:
# - Ejecutar Cross-Validation
# - Analizar m√©tricas promedio
# - Identificar el mejor modelo
# - Evaluarlo en el set de test

print("Entrenando modelos con Cross-Validation...")
cv_model = crossval.fit(train)
print("‚úì Cross-validation completada")

# %%
# Analizar m√©tricas promedio (RMSE) por configuraci√≥n
avg_metrics = cv_model.avgMetrics

# √çndice del mejor modelo (menor RMSE)
best_metric_idx = avg_metrics.index(min(avg_metrics))

print("\n=== M√âTRICAS PROMEDIO POR CONFIGURACI√ìN (RMSE) ===")
for i, metric in enumerate(avg_metrics):
    params = param_grid[i]
    reg = params[lr.regParam]
    elastic = params[lr.elasticNetParam]

    marker = " <-- MEJOR MODELO" if i == best_metric_idx else ""
    print(
        f"Config {i+1:02d} | "
        f"Œª={reg:<5.2f} | "
        f"Œ±={elastic:<3.1f} | "
        f"RMSE CV={metric:,.2f}"
        f"{marker}"
    )

# %%
# Obtener el mejor modelo encontrado por Cross-Validation
best_model = cv_model.bestModel

print("\n=== MEJOR MODELO SELECCIONADO ===")
print(f"regParam (Œª):        {best_model.getRegParam()}")
print(f"elasticNetParam (Œ±): {best_model.getElasticNetParam()}")

# %%
# Evaluar el mejor modelo en el set de test
predictions = best_model.transform(test)
rmse_test = evaluator.evaluate(predictions)

print("\n=== EVALUACI√ìN FINAL EN TEST ===")
print(f"RMSE Test: ${rmse_test:,.2f}")

# %%
# Comentario conceptual (para el notebook):
#
# - avgMetrics contiene el RMSE promedio de cada combinaci√≥n
# - El mejor modelo NO se elige por train, sino por validaci√≥n cruzada
# - Esto reduce overfitting y mejora generalizaci√≥n
#
# ‚úîÔ∏è El modelo seleccionado es el que minimiza el RMSE promedio en CV


# %%
# RETO 6: Comparar Cross-Validation vs Simple Split
#
# Objetivo:
# - Entrenar un modelo SIN Cross-Validation
# - Comparar su desempe√±o contra el modelo seleccionado con CV
# - Analizar cu√°l enfoque es m√°s confiable

from pyspark.ml.regression import LinearRegression

print("\nEntrenando modelo SIMPLE (sin Cross-Validation)...")

# Modelo simple usando los mismos hiperpar√°metros del mejor modelo CV
lr_simple = LinearRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=100,
    regParam=best_model.getRegParam(),
    elasticNetParam=best_model.getElasticNetParam()
)

# Entrenamiento
model_simple = lr_simple.fit(train)

# Evaluaci√≥n en test
rmse_simple = evaluator.evaluate(model_simple.transform(test))

# Comparaci√≥n
print("\n=== COMPARACI√ìN CV vs SIMPLE SPLIT ===")
print(f"RMSE con Cross-Validation: ${rmse_test:,.2f}")
print(f"RMSE sin Cross-Validation: ${rmse_simple:,.2f}")
print(f"Diferencia absoluta:       ${abs(rmse_test - rmse_simple):,.2f}")

# %%
# Interpretaci√≥n (completa como comentario en tu notebook):
#
# - El modelo con Cross-Validation es m√°s confiable porque:
#   ‚Ä¢ Eval√∫a m√∫ltiples particiones del train
#   ‚Ä¢ Reduce la dependencia de un solo split aleatorio
#   ‚Ä¢ Produce una m√©trica m√°s estable y robusta
#
# - El modelo sin CV puede:
#   ‚Ä¢ Verse afectado por la casualidad del split
#   ‚Ä¢ Sobreestimar o subestimar el rendimiento real
#
# Conclusi√≥n:
# ‚úîÔ∏è Cross-Validation ofrece una mejor estimaci√≥n del desempe√±o real del modelo


# %%
# RETO BONUS: Experimentar con diferentes valores de K (Cross-Validation)
#
# Objetivo:
# - Comparar K=3, K=5 y K=10
# - Observar impacto en RMSE y tiempo de ejecuci√≥n
# - Entender el trade-off entre robustez y costo computacional

from pyspark.ml.tuning import CrossValidator
import time

print("\n=== EXPERIMENTO CON DIFERENTES VALORES DE K ===")

resultados_k = []

for k in [3, 5, 10]:
    print(f"\nEjecutando Cross-Validation con K={k} folds...")

    cv_temp = CrossValidator(
        estimator=lr,
        estimatorParamMaps=param_grid,
        evaluator=evaluator,
        numFolds=k,
        seed=42
    )

    start_time = time.time()
    cv_temp_model = cv_temp.fit(train)
    elapsed_time = time.time() - start_time

    best_rmse = min(cv_temp_model.avgMetrics)

    resultados_k.append({
        "K": k,
        "best_rmse": best_rmse,
        "time_seconds": elapsed_time
    })

    print(
        f"K={k:2d} | "
        f"Mejor RMSE: ${best_rmse:,.2f} | "
        f"Tiempo: {elapsed_time:.1f} segundos"
    )

# %%
# Resumen comparativo
print("\n=== RESUMEN COMPARATIVO POR K ===")
for r in resultados_k:
    print(
        f"K={r['K']:2d} | "
        f"RMSE: ${r['best_rmse']:,.2f} | "
        f"Tiempo: {r['time_seconds']:.1f}s"
    )

# %%
# Interpretaci√≥n (completa como comentario en tu notebook):
#
# - K peque√±o (ej. 3):
#   ‚Ä¢ M√°s r√°pido
#   ‚Ä¢ M√©trica menos estable
#
# - K intermedio (5):
#   ‚Ä¢ Buen balance entre costo y robustez
#   ‚Ä¢ Opci√≥n m√°s com√∫n en pr√°ctica
#
# - K grande (10):
#   ‚Ä¢ M√©trica m√°s robusta
#   ‚Ä¢ Mucho m√°s costoso computacionalmente
#
# Conclusi√≥n:
# ‚ùå M√°s folds NO siempre es mejor
# ‚úîÔ∏è El valor √≥ptimo de K depende del tama√±o del dataset y del costo computacional

# %%
# Guardar el mejor modelo entrenado con Cross-Validation
model_path = "/opt/spark-data/processed/cv_best_model"

# Guardar modelo
best_model.save(model_path)

print(f"‚úì Modelo guardado correctamente en: {model_path}")


# %%
print("\n" + "="*60)
print("RESUMEN VALIDACI√ìN CRUZADA")
print("="*60)
print("Verifica que hayas completado:")
print("  [ ] Entendido el concepto de K-Fold")
print("  [ ] Configurado ParamGrid con hiperpar√°metros")
print("  [ ] Ejecutado CrossValidator")
print("  [ ] Identificado el mejor modelo")
print("  [ ] Comparado con entrenamiento simple")
print("="*60)

# %%
spark.stop()
